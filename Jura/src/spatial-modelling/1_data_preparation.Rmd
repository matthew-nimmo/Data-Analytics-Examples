---
format: html-eval-false
---

::: {.content-visible when-format="html-eval-false"}
```{r}
#| echo: false
#| output: false
library(targets)
```
:::

# Data

## Boundaries and geological domains

A boundary perimeter was provided as a GIS shape file. The boundary delimits the extent of the Mineral Resource estimate beyond the area covered by the sampling.

```{targets jura_bnd}
#| label: a_titanic_train
#| tar_simple: true
#| tar_interactive: false
#| output: false

rgdal::readOGR("data/bound.shp")
```

## Sample data

All sample data were provided in a single comma delimited (CSV) file. The data set contains assays for cadmium (Cd ppm), cobalt (Co ppm), chromium (Cr ppm), copper (Cu %), nickel (Ni ppm), lead (Pb ppm) and zinc (Zn ppm).

Automatic data type identification was utilized during importing of the nodule sample data from comma delimited and Microsoft Excel files. Conversion of the assay values results in values at or below or above detection limit (values prefixed with < or >) are set to detection limit (leading < or > are automatically removed). This is the preferred behaviour as values less than or above detection limit are censored data and it is not known by how much the value is below or above the detection limit. To avoid bias it is better to set the values to detection limit.

```{targets a_jura}
read.csv(
  "data/jura_modified.csv",
  header = TRUE,
  na.strings = c("NA","","-","-9"),
  stringsAsFactors = FALSE)
```

Spatial distribution of the sample data is shown in Figure \@ref(fig:ah-samp).

```{r ah-samp, fig.width=4, fig.height=4, fig.cap="Jura sample locations."}
z <- factor(df$rock)
plot(jura_bnd,
     col = "red",
     lwd = 2,
     xlab = "UTM X (km)",
     ylab = "UTM Y (km)",
     asp = 1)
points(jura$X,
     jura$Y,
     pch = 16,
     cex = 0.8,
     col = unclass(z))
legend("topleft",
       legend = levels(z),
       col = 1:nlevels(z),
       pch = 16,
       cex = 0.6)
rm(z)
```

The sample data provided by Kings Landing include `r nrow(jura)` samples. The sampling and assaying method is not known. Summary statistics of the primary assays are listed in Table \@ref(tab:ah-sumstat).

```{r ah-sumstat}
source("src/sum_stats.R")
source("src/digits.R")
flds <- names(jura)[sapply(jura, is.numeric)]
x <- as.data.frame(t(sapply(jura[, flds], sum_stats, simplify="data.frame")))
knitr::kable(x, format="markdown", caption="Assay summary statistics.")
rm(x, flds)
```

<br>

```{r}
flds_est <- c("cd_ppm","co_ppm","cr_ppm","cu_pct",
              "ni_ppm","pb_ppm","zn_ppm")
txt <- paste(flds_est, collapse=", ")
txt <- gsub("([,] )([^,]*)$", " and \\2", txt)
```

The assay variables that were used for resource estimation include `r txt`.

The cumulative probability plots of the sample assay data within the Jura copper deposit are provided in (Figure \@ref(fig:ah-cumprob)).

```{r ah-cumprob, fig.width=9, fig.height=9, fig.cap="Cumulative probability plots of sample assay data."}
par(mfrow=c(3,3), oma=c(1,0,0,0))
source("src/cumprob.R")
for (i in flds_est) {
  cumprob(as.formula(paste(i, "~ rock")), data=jura,
          main="", xlab=i, pch=16, cex=0.9)
}
```

The cumulative probability plots of the sample assay data within the Jura copper deposit are provided in (Figure \@ref(fig:ah-box)).

> Describe the plots.

```{r ah-box, fig.width=7, fig.height=7, fig.cap="Boxplots of assays by rock."}
par(mfrow=c(3,3), mar=c(5.1,6.1,1.1,1.1))

for (i in flds_est) {
  boxplot(as.formula(paste(i, "~ rock")), data=jura,
          horizontal=TRUE, pch=16, cex=0.9, frame.plot=FALSE, las=2,
          main="", ylab="", xlab=i)
}
```

### Sample clustering

The sample data contains a number of clusters. A cell declustering algorithm was applied to the sample locations. The spatially weighted means and unweighted means for the assays are shown in Table \@ref(tab:ah-cmeans). The weighted mean values can be used to compare with the block model estimated values for validation of grade estimation.

# Data preparation

```{r}
jura <- dplyr::select(
  jura, X=Xloc, Y=Yloc, rock=Rock, cd_ppm=Cd, co_ppm=Co,
  cr_ppm=Cr, cu_pct=Cu, ni_ppm=Ni, pb_ppm=Pb, zn_ppm=Zn)
jura$SEQ <- ifelse(jura$rock == "SEQ", 1, 0)
jura$KIM <- ifelse(jura$rock == "KIM", 1, 0)
jura$QUA <- ifelse(jura$rock == "QUA", 1, 0)
jura$ARG <- ifelse(jura$rock == "ARG", 1, 0)
jura$POR <- ifelse(jura$rock == "POR", 1, 0)
```

> Discuss results of comparison of means.

```{r ah-cmeans}
x <- sapply(jura[,flds_est], mean, na.rm=TRUE)
x <- round(x, 2)
source("src/declus.R")
y <- sapply(flds_est, function(i) {
  j <- !is.na(jura[[i]])
  wts <- declus(jura[j,i], x=jura$X[j], y=jura$Y[j],
                yanis=1.0, zanis=1.0, ncell=10,
                cmin=0.2, cmax=5, noff=1, minmax=0)
  wts <- ifelse(j, wts, 0)
  sum(jura[j,i] * wts, na.rm=TRUE) / sum(wts, na.rm=TRUE)
})
y <- round(y, 2)

z <- data.frame(mean=x, `wtd mean`=y)
knitr::kable(z, caption="Assay mean and declustered mean.")
rm(x, y, z)
```

<br>

### Outliers within the sample data

Outliers (extreme values) detected using the Local Outlier Factor algorithm are highlighted in the pairs plot shown in Figure \@ref(fig:ah-pairs) (red crosses) and listed in Table \@ref(tab:ah-out1). The spatial location of the identified outliers is shown in Figure \@ref(fig:ah-out2)

> Discuss plot.

```{r ah-pairs, fig.width=6.6, fig.height=6.6, fig.cap="Pairs plots showing identified outliers."}
panel.points2 <- function(x, y, ...)
{
  z <- densCols(x, y, colramp=colorRampPalette(c("black", "white")))
  cols <- viridis(256)

  df <- data.frame(x, y)
  df$dens <- col2rgb(z)[1,] + 1L
  df$col <- cols[df$dens]
  o <- order(df$dens)
  df <- df[o,]

  out <- rep(FALSE, nrow(df))
  out[outliers] <- TRUE
  out <- out[o]

  points(y~x, data=df[!out,], pch=20, col=col, cex=1.2)
  if (any(out)) {
    points(y~x, data=df[out,], pch=3, col="black", cex=1.2, lwd=1)
  }
}

x <- jura[, flds_est]

indx <- complete.cases(x)
outlier.scores <- rep(0, nrow(x))
outlier.scores[indx] <- lofactor(x[indx, ], k=5)
outliers <- order(outlier.scores, decreasing=TRUE)[1:5]

n <- nrow(x)
pch <- rep(19, n)
pch[outliers] <- 3
col <- rep("black", n)
col[outliers] <- "red"
cex <- rep(0.8, n)
cex[outliers] <- 1

pairs(x, cex=cex, gap=0.5, pch=pch, col=col,
      upper.panel=panel.cor, lower.panel=panel.points2)
rm(x, indx, n, pch, col, cex)
```

```{r ah-out1}
flds <- c("X","Y","rock", flds_est)
x <- B__AH[sort(outliers), flds]
#row.names(x) <- outliers
kable(x, format="markdown", digit=1, row.names=TRUE,
      caption="List of identified outliers.")
rm(x)
```

<br>

```{r ah-out2, fig.width=4, fig.height=4, fig.cap="Location of identified outliers."}
par(mar=c(5.1,4.1,1.1,1.1))

i <- 1:nrow(B__AH) %in% outliers 
plot(B__AH$X[!i], B__AH$Y[!i], col="black", pch=16, cex=0.6,
     xlab="UTM X (m)", ylab="UTM Y (m)", asp=1, frame.plot=FALSE)
points(B__AH$X[i], B__AH$Y[i], pch=19, cex=0.8, col="red")
lines(jura_bnd, col="red", lwd=2)
legend("topleft", "Identified potential outliers", horiz=TRUE,
       col="red", pch=19, cex=0.7, bty="n", inset=c(0, -0.06), xpd=TRUE)

rm(i)
```

### Top-cuts

To help minimise the influence of outliers on the estimation of local grades the most common approach is to trim those outliers to some arbitrary value (typically the 97.5, 98 or 99 percentile or mean plus twice standard deviation). Table \@ref(tab:ah-topcut) lists the top-cuts that could be considered using these methods. Other methods include selecting the trim value by inspecting log-probability plots and looking for where the distribution breaks down in the high values, or looking at the inflection point in a mean-variance plot.

```{r ah-topcut}
func <- function(x) {
  y <- na.omit(x)
  v <- c(quantile(y, probs=c(0.975,0.98,0.99)),
         mean(y)+2*sd(y), median(y)+1.5*IQR(y))
  v <- round(v, 1)
  names(v) <- c("97.5%","98.0%","99.0%","Mean+2*sd","Median+1.5*IQR")
  return(v)
}

x <- t(sapply(B__AH[, flds_est], func))
x <- as.data.frame(x)
kable(x, format="markdown", caption="List of possilbe top-cuts for each assay.")
rm(x)
```

<br>

The selected top-cuts (values to trim the sample assays by) applied to the sample data are listed in Table \@ref(tab:top-cuts1) and are shown as a red line in the plots provided in Figure \@ref(fig:top-cuts2). The mean-variance curves (Figure \@ref(fig:mean-var)) by top-cut value show that the selected top-cut values occur where at the inflection point in mean curve where decreasing top-cut would result in a rapidly decreasing mean value. Above the top-cut values the means are relatively stable.

```{r top-cuts1}
top_cuts <- data.frame(Variable=flds_est,
                       Cut=c(3.8,15.2,59.1,16.5,35.2,144,146))
kable(top_cuts, caption="Assay top-cut values.")
```

<br>

```{r top-cuts2, fig.width=6, fig.height=2, fig.cap="Histogram, log-probability and mean value byplots to assess top-cuts."}
func <- function(cutval, x) {
  y <- ifelse(x > cutval, cutval, x)
  v <- c(Mean=mean(y), Var=var(y))
}

flds <- top_cuts$Variable
cutval <- top_cuts$Cut
names(cutval) <- flds

#par(mar=c(5.1,4.1,2.1,4.1), mfrow=c(5, 3), cex=1)

for (i in flds) {
  par(mfcol=c(1,3), mar=c(6.1,4.1,2.1,2.1))

  x <- na.omit(B__AH[[i]])
  cv <- cutval[[i]]

  hist(x, breaks="FD", main="", xlab=i)
  abline(v=cv, col="red")
  mtext(cv, side=3, line=0, at=cv, cex=0.55, col="red")

  cumprob(x, main="", xlab=i)
  abline(v=cv, col="red")
  mtext(cv, side=3, line=0, at=cv, cex=0.55, col="red")

  par(mar=c(6.1,4.1,3.1,5.1))
  q <- round(quantile(x, probs=(75:100)/100), 2)
  y <- sapply(q, func, x)
  plot(q, y[1,], type="l", col="black", frame.plot=FALSE,
       xlab="Trim value", ylab="Mean", main="")
  abline(v=cv, col="red")
  mtext(cv, side=3, line=0, at=cv, cex=0.55, col="red")
  par(new=TRUE)
  plot(q, y[2,], type="l", col="blue", axes=FALSE, xlab="", ylab="")
  axis(side=4)
  mtext(side=4, line=3, "Variance", col="blue", cex=0.7)
  title(main=i, line=2)
}

rm(flds, cutval, x, cv, q, y, i)
```

### Missing value imputation

## Representativeness of sampling
